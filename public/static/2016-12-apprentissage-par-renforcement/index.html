<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
	<link rel="icon" href="data:," />
	<meta http-equiv="content-type" content="text/html; charset=utf-8"/>
	<title>Apprentissage par renforcement</title>
	<meta name="generator" content="LibreOffice 6.2.8.2 (Linux)"/>
	<meta name="created" content="2016-12-21T21:40:53.568743156"/>
	<meta name="changed" content="2017-01-11T15:37:01.813533628"/>
	<style type="text/css">
		@page { size: 8.27in 11.69in; margin: 0.79in }
		@page:first { }
		p { margin-bottom: 0.1in; direction: ltr; line-height: 110%; text-align: justify; background: transparent; page-break-before: auto }
		p.western { font-family: "CMU Serif"; font-weight: normal }
		p.cjk { font-size: 10pt }
		h1 { margin-bottom: 0.08in; direction: ltr; background: transparent; page-break-after: avoid }
		h1.western { font-family: "CMU Sans Serif"; font-size: 18pt; font-weight: bold }
		h1.cjk { font-family: "Source Han Sans CN Regular"; font-size: 18pt; font-weight: bold }
		h1.ctl { font-family: "DejaVu Sans"; font-size: 18pt; font-weight: bold }
		h2 { margin-top: 0.14in; margin-bottom: 0.08in; direction: ltr; background: transparent; page-break-after: avoid }
		h2.western { font-family: "CMU Sans Serif"; font-size: 16pt; font-weight: bold }
		h2.cjk { font-family: "Source Han Sans CN Regular"; font-size: 16pt; font-weight: bold }
		h2.ctl { font-family: "DejaVu Sans"; font-size: 16pt; font-weight: bold }
		h3 { margin-top: 0.1in; margin-bottom: 0.08in; direction: ltr; background: transparent; page-break-after: avoid }
		h3.western { font-family: "CMU Sans Serif"; font-size: 14pt; font-weight: bold }
		h3.cjk { font-family: "Source Han Sans CN Regular"; font-size: 14pt; font-weight: bold }
		h3.ctl { font-family: "DejaVu Sans"; font-size: 14pt; font-weight: bold }
		a:link { color: #000080; so-language: zxx; text-decoration: underline }
		a:visited { color: #800000; so-language: zxx; text-decoration: underline }
	</style>
</head>
<body lang="fr-FR" link="#000080" vlink="#800000" dir="ltr"><p class="western" align="center" style="margin-bottom: 0in; line-height: 100%">
<font size="4" style="font-size: 14pt">L’école nationale
supérieure d’informatique</font></p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
Décembre 2016</p>

</p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<br/>

</p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 70pt; page-break-before: auto">
<font size="7" style="font-size: 60pt">Apprentissage</font></p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<font size="7" style="font-size: 60pt">par renforcement</font></p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<br/>

</p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<font color="#666666"><font size="5" style="font-size: 18pt">Séminaire
doctoral d’apprentissage machine</font></font></p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<br/>

</p>

<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<font size="4" style="font-size: 16pt"><b>	Réalisé par&nbsp;:</b></font></p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<font size="4" style="font-size: 16pt">		Nawfel BENGHERBIA</font></p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<font size="4" style="font-size: 16pt">		1ère année doctorat au
CERIST</font></p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<br/>

</p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<font size="4" style="font-size: 16pt"><b>	Pour&nbsp;:</b></font></p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<font size="4" style="font-size: 16pt">		Mme Leila HAMDAD</font></p>

<p><a href="./rapport.pdf">Télécharger au format PDF</a></p>

<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<br/>
</p>

<div id="Table of Contents1" dir="ltr">
	<div id="Table of Contents1_Head" dir="ltr"><p align="left" style="margin-top: 0.17in; margin-bottom: 0.08in; line-height: 100%; page-break-before: always; page-break-after: avoid">
		<font face="CMU Sans Serif"><font size="4" style="font-size: 16pt"><b>Table
		des matières</b></font></font></p>
	</div>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1013_2125244816">1.Introduction	2</a></p>
	<p align="left" style="margin-left: 0.2in; margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc736_1726350269">1.1.Exemples
	d’applications	2</a></p>
	<p align="left" style="margin-left: 0.2in; margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc738_1726350269">1.2.Challenges	2</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1015_2125244816">2.Modélisation de
	l’environnement	4</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1017_2125244816">3.Politique et valeur
	d’une politique	5</a></p>
	<p align="left" style="margin-left: 0.2in; margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1019_2125244816">3.1.Valeur d'un couple
	état-action pour une politique donnée	5</a></p>
	<p align="left" style="margin-left: 0.2in; margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1073_2125244816">3.2.Équations de
	Bellman	6</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1563_405684773">4.Problème de
	planification	7</a></p>
	<p align="left" style="margin-left: 0.2in; margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1565_405684773">4.1.Prédiction	7</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	      <a href="#__RefHeading___Toc1567_405684773">Algorithme
	d’estimation de la fonction q</a><a href="#__RefHeading___Toc1567_405684773"><sub>π</sub></a><a href="#__RefHeading___Toc1567_405684773">	7</a></p>
	<p align="left" style="margin-left: 0.2in; margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1569_405684773">4.2.Contrôle	8</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	      <a href="#__RefHeading___Toc1571_405684773">Algorithme
	d’itération de politiques (estimation de q</a><a href="#__RefHeading___Toc1571_405684773"><sub>*</sub></a><a href="#__RefHeading___Toc1571_405684773">)	8</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	      <a href="#__RefHeading___Toc1573_405684773">Itération de
	politiques généralisée	8</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1968_405684773">5.Apprentissage par
	renforcement	10</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	      <a href="#__RefHeading___Toc1970_405684773">L’algorithme
	SARSA	10</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1972_405684773">6.Apprentissage par
	renforcement et approximation de fonctions	12</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	      <a href="#__RefHeading___Toc2030_405684773">L’algorithme
	SARSA avec fonction d’approximation dérivable	13</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1523_834395304">7.SATSA(λ) avec traces
	d’éligibilité	15</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1441_1845084985">8.Application sur R	16</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1525_834395304">9.Application
	Javascript	16</a></p>
	<p align="left" style="margin-left: 0.2in; margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1654_881107470">9.1.Modélisation	16</a></p>
	<p align="left" style="margin-left: 0.2in; margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1735_881107470">9.2.Résultat	18</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1656_881107470">10.Logiciels
	d’apprentissage par renforcement	18</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1737_881107470">11.Conclusion	19</a></p>
	<p align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<a href="#__RefHeading___Toc1658_881107470">12.Références	20</a></p>
</div>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<br/>

</p>
<h1 class="western" style="page-break-before: always"><a name="__RefHeading___Toc1013_2125244816"></a>
1.Introduction</h1>
<p class="western">L’apprentissage par renforcement est un sous
domaine de l’apprentissage machine où un agent interagit avec son
environnement et apprend à choisir les actions qui maximisent ses
gains.</p>
<p class="western"><a name="result_box"></a>L’apprentissage par
renforcement est différent de l’apprentissage supervisé du fait
que l’agent n’obtient pas, en interagissant avec l’environnement,
des exemples (état, action à suivre). Il obtient plutôt, suite au
choix d’une action, un gain sans savoir si <span lang="fr-FR">le
gain aurait pu être meilleur ou pire avait-il choisi une autre
action.</span></p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_347ef54c086aa71e.gif" name="Frame1" alt="Frame1" align="bottom"/>
</p>
<h2 class="western"><a name="__RefHeading___Toc736_1726350269"></a>1.1.Exemples
d’applications</h2>
<ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Robots qui apprennent tous seuls à se mobiliser pour atteindre un
	but&nbsp;: <a href="https://youtu.be/ggqnxyjaKe4?t=292">https://youtu.be/ggqnxyjaKe4?t=292</a></p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Programmes qui jouent au jeux Atari avec une performance compétitive
	aux êtres humains&nbsp;: <a href="https://youtu.be/ggqnxyjaKe4?t=784">https://youtu.be/ggqnxyjaKe4?t=784</a></p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	AlphaGo: un programme qui joue au jeu de go, et qui a battu le
	champion du monde  
	<a href="https://research.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html">https://research.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html</a>
		</p>
</ul>
<h2 class="western"><a name="__RefHeading___Toc738_1726350269"></a>1.2.Challenges</h2>
<p class="western">Parmi les chalenges d’apprentissage par
renforcement, on cite&nbsp;:</p>
<ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Chercher une balance entre l’exploitation et l’exploration&nbsp;:
	Le besoin de choisir des actions avec de bonnes récompenses et le
	besoin d’exploration pour découvrir s’il y a d’autres actions
	de meilleures récompenses.</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Le fait que les actions peuvent avoir des conséquences non
	immédiates (par exemple&nbsp;: une très grande récompense suite à
	une chaine d’actions avec récompenses petites relativement à
	leurs alternatifs).</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Dans les problèmes du monde réel, le nombre des états possibles
	de l’environnement peut être prohibitivement grand pour être
	encodés sur ordinateur. Ceci impose l’utilisation des fonctions
	d’approximation comme les réseaux de neurones artificiels et les
	arbres de décisions.</p>
</ul>
<h1 class="western" style="page-break-before: always"><a name="__RefHeading___Toc1015_2125244816"></a>
2.Modélisation de l’environnement</h1>
<p class="western">On s’intéresse à un agent qui interagit avec
son environnement. L’agent observe l’état de l’environnement
et choisi l’action à faire. L’environnement récompense l’agent
ou le punit avec un signal (gain). L’agent choisi ses actions de
façon à maximiser ses gains à long terme.</p>
<p class="western">On suppose que l’interaction entre l’agent et
l’environnement peut être décrite par un processus de décision
Markovien (MDP&nbsp;: Markov Decision Process). Un tel processus est
décrit par&nbsp;:</p>
<ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Un ensemble fini d’états s ∈ S&nbsp;;</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Un ensemble fini d’actions a ∈ A&nbsp;;</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Un ensemble fini de valeurs de gain r ∈ R&nbsp;;</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	La dynamique du système est Markovienne: La probabilité d’obtenir
	un gain r et arriver à l’état s’ au moment t+1 dépend
	seulement de l’état et l’action entreprise au moment t&nbsp;:</p>
	<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<img src="RL_html_5850ea8e372e4667.gif" name="Object20" hspace="8" width="473" height="42"/>
</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Si les probabilités<img src="RL_html_23039d97d748e055.gif" name="Object18" hspace="8" width="105" height="19"/>
sont
	connues, on les stocke dans un tableau P.</p>
</ul>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<br/>

</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_39dbfc45804da54.gif" name="Frame2" alt="Frame2" align="bottom"/>
</p>
<h1 class="western"><a name="__RefHeading___Toc1017_2125244816"></a>3.Politique
et valeur d’une politique</h1>
<p class="western">Une politique π défini le comportement de
l’agent. On définie la fonction 
<img src="RL_html_939364e6629e5e78.gif" name="Object10" hspace="8" width="54" height="19"/>
comme
étant la probabilité que l’agent choisit l’action a lorsqu’il
est dans l’état s, sous la politique  π&nbsp;:</p>
<p class="western" align="center"><img src="RL_html_b33981b06f66b2b.gif" name="Object1" hspace="8" width="317" height="33"/>
</p>
<p class="western">Si la politique	associe à l’état s l’action
a avec probabilité 1, on se permet d’écrire&nbsp;:</p>
<p class="western" align="center"><img src="RL_html_d6028c74c6bb2639.gif" name="Object5" hspace="8" width="59" height="19"/>
</p>
<p class="western">Si à tout état la politique associe une seule
action, on dit qu’elle est déterministe.</p>
<h2 class="western"><a name="__RefHeading___Toc1019_2125244816"></a>3.1.Valeur
d'un couple état-action pour une politique donnée</h2>
<p class="western">On défini la fonction valeur d’état-action:<img src="RL_html_dfcb63efc104356b.gif" name="Object65" hspace="8" width="59" height="20"/>
,
comme étant le gain à long terme que l’agent espère recevoir en
étant à l’état s, en faisant l’action a et en suivant par la
suite la politique π.</p>
<p class="western">Si on est à l’instant t, on défini le gain à
long terme par la somme&nbsp;:</p>
<p class="western" align="center"><img src="RL_html_705892c4eda685ba.gif" name="Object21" hspace="8" width="208" height="22"/>
</p>
<p class="western">où<img src="RL_html_352dfdbda0b25d58.gif" name="Object66" hspace="8" width="33" height="20"/>
est
le gain que l’agent obtiendra en faisant l’action<img src="RL_html_a795ba0d80b87228.gif" name="Object67" hspace="8" width="23" height="20"/>
et
γ (0 ≤ γ &lt; 1) s’appelle le facteur de décompte. Plus ceci
est proche de 0, plus on valorise les gains immédiats par rapport
aux gains futurs.</p>
<p class="western">la fonction valeur d’état-action<img src="RL_html_dfcb63efc104356b.gif" name="Object68" hspace="8" width="59" height="20"/>
est
définie par&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_a253059792efb489.gif" name="Object22" hspace="8" width="411" height="22"/>
</p>
<p class="western">On dit que la politique π’ et meilleure que la
politique π ssi&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_6f9899d201f80799.gif" name="Object2" hspace="8" width="237" height="20"/>
</p>
<p class="western">Pour tout MDP, il existe au moins une politique
optimale déterministe.</p>
<p class="western">Une politique π<sub>*</sub> est optimale si elle
maximise la fonction valeur d’état-action. c’est-à-dire&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_29eeb557998a9263.gif" name="Object3" hspace="8" width="252" height="29"/>
</p>
<p class="western">Étant donné que le nombre de politiques est
exponentiel en nombre d’états, il peut semblé inenvisageable de
trouver cette q<sub>*</sub> . Heureusement pour nous, le théorème
d’amélioration de politiques, qu’on verra plus tard, nous donne
exactement ça.</p>
<p class="western">Étant donné q<sub>*</sub>, il est facile de se
comporter de façon optimale. Il suffit de choisir à tout moment
l’action du gain maximal on choisissant à chaque état l’action
de valeur maximale&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_49e7e356892b967a.gif" name="Object4" hspace="8" width="168" height="29"/>
</p>
<p class="western">Il existe donc toujours au moins une politique
optimale déterministe.</p>
<h2 class="western"><a name="__RefHeading___Toc1073_2125244816"></a>3.2.Équations
de Bellman</h2>
<p class="western">D’après l’équation d’expectation de
Bellman, la fonction valeur d’état-action<img src="RL_html_dfcb63efc104356b.gif" name="Object69" hspace="8" width="59" height="20"/>
d’une
politique π peut s’écrire comme&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_243deee1a67629bb.gif" name="Object6" hspace="8" width="389" height="21"/>
</p>
<p class="western">Quant à la fonction valeur optimale q<sub>*</sub>,
l’équation d’optimalité de Bellman nous donne&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_1758f6e048849c20.gif" name="Object7" hspace="8" width="371" height="30"/>
</p>
<h1 class="western" style="page-break-before: always"><a name="__RefHeading___Toc1563_405684773"></a>
4.Problème de planification</h1>
<p class="western">Le problème de planification est un cas
particulier de l’apprentissage par renforcement où on connaît le
modèle de l’environnement, c’est-à-dire&nbsp;: la probabilité
d’arriver à un état s’ et recevoir un gain r de n’importe
quel couple état-action (s, a)&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_cd3cc7ff8f3ad0b5.gif" name="Object8" hspace="8" width="436" height="19"/>
</p>
<p class="western">On étudie deux problèmes&nbsp;:</p>
<ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<span style="font-style: normal"><b>Prédiction</b></span>&nbsp;:
	Déterminer la fonction valeur d’état-action pour une politique
	donné&nbsp;;</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<b>Contrôle</b>&nbsp;: Rechercher la politique optimale d’un MDP.</p>
</ul>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<br/>

</p>
<h2 class="western"><a name="__RefHeading___Toc1565_405684773"></a>4.1.Prédiction</h2>
<p class="western">Une façon de calculer (ou estimer) la fonction q<sub>π</sub>
est de l’écrire en fonction de lui même et puis utiliser la
programmation dynamique. On prend l’équation d’expectation de
Bellman, et on évalue l’espérance mathématique en utilisant le
modèle de l’environnement&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_3f15347aec184b2d.gif" name="Object9" hspace="8" width="528" height="88"/>
</p>
<p class="western">On crée un tableau<img src="RL_html_6ab18ee8af1aa89c.gif" name="Object72" hspace="8" width="56" height="19"/>
qui
va représenter la fonction<img src="RL_html_dfcb63efc104356b.gif" name="Object71" hspace="8" width="59" height="20"/>
,
et on l’initialise aléatoirement. Ensuite on met à jour les
entrées du tableau en utilisant la relation récursive. On répète
ce processus itérativement. À la limite, un théorème nous dit
que<img src="RL_html_6ab18ee8af1aa89c.gif" name="Object73" hspace="8" width="56" height="19"/>
convergera
vers<img src="RL_html_dfcb63efc104356b.gif" name="Object70" hspace="8" width="59" height="20"/>
.</p>
<h3 class="western"><a name="__RefHeading___Toc1567_405684773"></a>Algorithme
d’estimation de la fonction q<sub>π</sub></h3>
<ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Entrées&nbsp;: (S, A, P, π)</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Sortie&nbsp;:Un tableau  Q(s, a)</p>
	<ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Initialiser le tableau&nbsp;: Q(s, a) ← 0 pour tout s, a&nbsp;;</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Répéter&nbsp;:</p>
		<ul>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Δ ←0</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Pour chaque couple état-action (s, a), fait&nbsp;:</p>
			<ul>
				<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
				q ← Q(s, a)&nbsp;;</p>
				<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
				Mise à jour&nbsp;:</p>
				<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
				<img src="RL_html_b79cf5a34a64eccf.gif" name="Object11" hspace="8" width="424" height="33"/>
</p>
				<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
				Δ ← max( Δ, |q – Q(s, a)| )&nbsp;;</p>
			</ul>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Fin pour.</p>
		</ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Jusqu’à (Δ &lt; un seuil).</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Retourner le tableau<img src="RL_html_8e362477924577b7.gif" name="Object16" hspace="8" width="120" height="20"/>
.</p>
	</ul>
</ul>
<h2 class="western"><a name="__RefHeading___Toc1569_405684773"></a>4.2.Contrôle</h2>
<p class="western">Le théorème d’amélioration de politiques nous
dit qu’étant donné une politique π avec fonction valeur
d’état-action<img src="RL_html_dfcb63efc104356b.gif" name="Object74" hspace="8" width="59" height="20"/>
,
on peut créer une meilleure politique π’ (<img src="RL_html_c5f26aa260036767.gif" name="Object13" hspace="8" width="125" height="20"/>
)
en choisissant dans la nouvelle politique π’ pour un état s
l’action maximisant<img src="RL_html_dfcb63efc104356b.gif" name="Object75" hspace="8" width="59" height="20"/>
&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_f824ed42d9a4f6bb.gif" name="Object12" hspace="8" width="174" height="29"/>
</p>
<p class="western">Ce résultat nous donne un algorithme pour trouver
la politique optimale, il suffit de commencer par une politique
quelconque π<sub>0</sub>, estimer<img src="RL_html_f24bf1959b6c3804.gif" name="Object76" hspace="8" width="62" height="22"/>
comme
décrit dans la section précédente, créer une politique améliorée
 π<sub>1</sub>, estimer<img src="RL_html_7597b305c528c195.gif" name="Object77" hspace="8" width="62" height="22"/>
,
améliorer π<sub>1</sub> et ainsi de suite. À la limite, on
convergera vers la politique optimale π<sub>*</sub>.</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_8d69362699b5ef14.gif" name="Object14" hspace="8" width="293" height="22"/>
</p>
<h3 class="western"><a name="__RefHeading___Toc1571_405684773"></a>Algorithme
d’itération de politiques (estimation de q<sub>*</sub>)</h3>
<ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Entrées&nbsp;: (S, A, P)</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Sortie&nbsp;: Un tableau Q(s, a)</p>
	<ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Créer aléatoirement une politique π déterministe&nbsp;;</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Répéter&nbsp;:</p>
		<ul>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			stable ← Vrai&nbsp;;</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Q(s, a) ← Algorithme d’estimation de la fonction q<sub>π</sub>(S,
			A, P(s’,r | s, a), π)</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Pour chaque<img src="RL_html_798ea2cb117fa7c6.gif" name="Object15" hspace="8" width="36" height="18"/>
,
			faire&nbsp;:</p>
			<ul>
				<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
				b ←  π(s)&nbsp;;</p>
				<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
				Amélioration de la politique&nbsp;:</p>
				<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
				<img src="RL_html_ed950ff8b2d3b6c.gif" name="Object19" hspace="8" width="168" height="29"/>
</p>
				<ul>
					<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
					en cas d'égalité (deux ou plusieurs actions de valeur
					maximale), on choisit une des actions toujours de la même
					façon)</p>
				</ul>
				<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
				Si b ≠ π(s) Alors stable ← Faux&nbsp;; Fin Si&nbsp;;</p>
			</ul>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Fin Pour.</p>
		</ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Jusqu’à (stable = Vrai).</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Retourner le tableau<img src="RL_html_b7299f4f7818311f.gif" name="Object17" hspace="8" width="119" height="20"/>
.</p>
	</ul>
</ul>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<br/>

</p>
<h3 class="western"><a name="__RefHeading___Toc1573_405684773"></a>Itération
de politiques généralisée</h3>
<p class="western">L’algorithme présenté ci-dessus est
inefficace. Il répète les deux phases (estimation et amélioration)
jusqu’à la convergence, alors que l’estimation en elle-même ne
converge qu’à l’infini. Dans la pratique, on remarque qu’il
suffit de faire quelques itérations d’estimation seulement pour
que notre Q(s, a) estimé aie suffisamment d’informations pour
améliorer la politique en cours.</p>
<p class="western">L’idée de <b>l’algorithme d’itération de
politiques généralisée </b><b>(GPI)</b> est d’entrelacer les
processus d’estimation et d’amélioration de façon quelconque.
Un théorème nous dit que ce nouvel algorithme converge lui aussi
vers la fonction valeur d’état-action optimale<img src="RL_html_8f9c5fdd2c3c3753.gif" name="Object78" hspace="8" width="58" height="20"/>
.
Les algorithmes qu’on verra dans les sections suivantes sont des
variations de l’itération de politiques généralisée.</p>
<h1 class="western" style="page-break-before: always"><a name="__RefHeading___Toc1968_405684773"></a>
5.Apprentissage par renforcement</h1>
<p class="western">On passe maintenant à un cas plus général de
l’apprentissage par renforcement. Dans cette partie, on suppose que
le modèle du MDP n’est pas connu&nbsp;: On ne connaît pas les
probabilités<img src="RL_html_59b89be5898a7422.gif" name="Object23" hspace="8" width="105" height="19"/>
.
Le but est que l’agent apprenne le comportement optimal tout en
interagissant avec son environnement. On veut que la performance de
l’agent s’améliore au fil du temps.</p>
<p class="western">On décrit ici l’algorithme SARSA qui fait
partie des méthodes d’itération de politiques généralisée
(GPI) qui entrelacent deux processus&nbsp;: un processus d’évaluation
et un processus d’amélioration de la politique en cours.</p>
<p class="western">Dans l’algorithme SARSA, l’agent apprend à
faire les bonnes actions tout en interagissant avec son
environnement. Il maintient un tableau Q(s, a)&nbsp;: une estimation
de la fonction<img src="RL_html_8f9c5fdd2c3c3753.gif" name="Object79" hspace="8" width="58" height="20"/>
,
et il suit une politique gloutonne (<b>ε-greedy</b>) qui choisi la
meilleur action selon Q(s, a) avec probabilité (1 – ε) (en vue de
maximiser les gains&nbsp;: <b>Exploiter</b> la politique apprise), ou
bien une autre action aléatoire avec probabilité ε (pour <b>explorer</b>
les autres actions).</p>
<p class="western">L’algorithme SARSA se base sur le principe de
<span style="font-style: normal"><b>Bootstrapping</b></span>&nbsp;:
Mettre à jour un estimateur à partir d’un autre estimateur. À
chaque interaction avec l’environnement, l’agent améliore son
estimation de<img src="RL_html_d45089715f10a488.gif" name="Object80" hspace="8" width="65" height="20"/>
en
utilisant<img src="RL_html_bfbab467b993d008.gif" name="Object81" hspace="8" width="88" height="20"/>
et
le gain observé<img src="RL_html_352dfdbda0b25d58.gif" name="Object82" hspace="8" width="33" height="20"/>
,
grâce à l’équation d’expectation de Bellman&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_75e366552545c937.gif" name="Object29" hspace="8" width="389" height="21"/>
</p>
<p class="western">Le nouvel estimateur de<img src="RL_html_d45089715f10a488.gif" name="Object83" hspace="8" width="65" height="20"/>
est
donné par&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_70132ea8237ec6e4.gif" name="Object30" hspace="8" width="138" height="21"/>
</p>
<h3 class="western"><a name="__RefHeading___Toc1970_405684773"></a>L’algorithme
SARSA</h3>
<ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Entrées&nbsp;: (S, A)</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	But&nbsp;: Trouver une politique quasi-optimale<img src="RL_html_1b694c179144ac59.gif" name="Object32" hspace="8" width="163" height="20"/>
.</p>
	<ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Initialiser un tableau Q(s, a) aléatoirement&nbsp;;</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		π  ← Une politique qui Choisit pour chaque état s,
		l’action<img src="RL_html_73e0269aa922a5f6.gif" name="Object27" hspace="8" width="107" height="29"/>
avec
		probabilité 
		<img src="RL_html_2b564895381cf0a3.gif" name="Object28" hspace="8" width="48" height="19"/>

		ou bien une action aléatoire avec probabilité ε.</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		s ← l’état initial&nbsp;;</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		a ← une action choisie de s par la politique π&nbsp;;</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Répéter&nbsp;:</p>
		<ul>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Effectuer l’action a et observer le gain r et l’état s’&nbsp;;</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			a’ ← une action choisie de s’ par la politique π&nbsp;;</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Calculer l’erreur entre la valeur prévue et le nouvel
			estimateur&nbsp;:</p>
			<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			<img src="RL_html_c3199fe4923d6300.gif" name="Object24" hspace="8" width="264" height="19"/>
</p>
			<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			<img src="RL_html_b2c8b2ffe15e3a72.gif" name="Object25" hspace="8" width="279" height="19"/>
</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Mettre à jour l’estimation de sorte à réduire l’erreur&nbsp;:</p>
			<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			<img src="RL_html_460971e43fe2b913.gif" name="Object26" hspace="8" width="220" height="19"/>
</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Faire décroitre la valeur de α&nbsp;;</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			s ← s’&nbsp;; a ← a’&nbsp;;</p>
		</ul>
	</ul>
</ul>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<br/>

</p>
<p class="western">Quand on met à jour une case du tableau Q(s, a),
on ne remplace pas l’ancienne estimation par la nouvelle. Mais on
la reproche de la nouvelle estimation avec un pas α ∈ [0, 1].</p>
<p class="western">Si 
<img src="RL_html_b35576c4fc721c1.gif" name="Object31" hspace="8" width="159" height="33"/>
,
l’algorithme SARSA est garanti de ne pas diverger de la politique
optimale (C’est-à-dire que l’erreur entre la fonction Q(s, a)
estimée et q<sub>*</sub>(s, a) est bornée).</p>
<h1 class="western" style="page-break-before: always"><a name="__RefHeading___Toc1972_405684773"></a>
6.Apprentissage par renforcement et approximation de fonctions</h1>
<p class="western">Dans les problèmes pratiques, on se trouve
souvent avec un nombre gigantesque d’états ce qui rend impossible
d’utiliser les méthodes tabulaires (manipulant le tableau Q(s, a))
par manque d’espace de stockage ou à cause de la lenteur d’accès
aux données. Heureusement, l’apprentissage par renforcement peut
être généralisé par le biais des fonctions d’approximation
comme les réseaux de neurones et les arbres de décisions. En
s’intéresse ici à l’approximation par les fonctions dérivables.</p>
<p class="western">On utilise au lieu de la fonction de valeur
d’état-action<img src="RL_html_74286023cf043283.gif" name="Object38" hspace="8" width="53" height="19"/>
,
sa fonction approximée 
<img src="RL_html_73d0910a5d8fb40e.gif" name="Object35" hspace="8" width="126" height="19"/>
où
<font face="CMU Sans Serif">θ</font> est le vecteur des paramètres
de la fonction d’approximation (Les paramètres d’un réseau de
neurones par exemple). Donc, au lieu de stocker le tableau Q(s, a),
on stocke uniquement le vecteur <font face="CMU Sans Serif">θ</font>.</p>
<p class="western">A tout état s, on capture quelques propriétés
de l’environnement pour créer un <b>feature vector</b> X(s). La
fonction 
<img src="RL_html_2828d4b28dd91941.gif" name="Object33" hspace="8" width="16" height="18"/>
utilise
ce vecteur pour calculer la valeur du couple état-action&nbsp;: 
<img src="RL_html_fd0dd430777ac9b2.gif" name="Object34" hspace="8" width="182" height="19"/>
.</p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
Le but de l’apprentissage devient de trouver le vecteur de
paramètres <font face="CMU Sans Serif">θ</font> qui minimise
l’erreur&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_2522ba98b60a152e.gif" name="Object36" hspace="8" width="241" height="22"/>
</p>
<p class="western">Comme <font face="CMU Sans Serif">θ</font> est de
taille très petite par rapport à Q(s, a), il est généralement
impossible de trouver un <font face="CMU Sans Serif">θ</font> tel
que l(<font face="CMU Sans Serif">θ</font>) = 0. De plus, si la
fonction<img src="RL_html_2828d4b28dd91941.gif" name="Object37" hspace="8" width="16" height="18"/>
n’est
pas linéaire, il devient difficile de trouver <font face="CMU Sans Serif">θ</font><sub>*</sub>&nbsp;:
l’optimum global de la fonction l(<font face="CMU Sans Serif">θ</font>).</p>
<p class="western"><img src="RL_html_cb8a6b7925125406.gif" name="Frame3" alt="Frame3" align="bottom"/>
</p>
<h3 class="western"><a name="__RefHeading___Toc2030_405684773"></a>L’algorithme
SARSA avec fonction d’approximation dérivable</h3>
<p class="western">Le principe de l’algorithme reste le même sauf
pour le tableau Q(s, a) qui n’existe plus. Au lieu de lire la
valeur d’une case du tableau Q(s, a), on invoque la fonction<img src="RL_html_8a1a3b14db8e5fb0.gif" name="Object40" hspace="8" width="68" height="19"/>
,
et au lieu de mettre à jour une case de Q(s, a) à la fois, on met à
jour tout le vecteur <font face="CMU Sans Serif">θ</font>, le but
étant de généraliser ce qu’on apprend à plusieurs couples
état-action. Tous les composantes du vecteur <font face="CMU Sans Serif">θ</font>
ne sont pas mise à jours de la même façon, mais en fonction de
leur contribution à l’erreur l(<font face="CMU Sans Serif">θ</font>),
c’est-à-dire que chaque composante <font face="CMU Sans Serif">θ</font><sub>i</sub>
est modifiée proportionnellement à<img src="RL_html_d1e88f3dd237d920.gif" name="Object41" hspace="8" width="73" height="20"/>
.</p>
<p class="western">Par application du principe de bootstrapping, et
suite à l’observation 
<img src="RL_html_86c14924fa424309.gif" name="Object44" hspace="8" width="260" height="20"/>
,<img src="RL_html_6a020424ca2bdd.gif" name="Object42" hspace="8" width="129" height="20"/>
devient
notre nouvel estimateur de<img src="RL_html_d4023f2288417e22.gif" name="Object43" hspace="8" width="64" height="20"/>
.
On met à jour le vecteur <font face="CMU Sans Serif">θ</font> comme
suit&nbsp;:</p>
<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_16d4d780c404f199.gif" name="Object39" hspace="8" width="597" height="99"/>
</p>
<p class="western">On suppose que notre nouvel estimateur<img src="RL_html_6a020424ca2bdd.gif" name="Object45" hspace="8" width="129" height="20"/>
est
indépendant de <font face="CMU Sans Serif">θ</font>, et on l’annule
par la dérivée partielle pour obtenir&nbsp;:</p>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<img src="RL_html_d116962afec688d4.gif" name="Object46" hspace="8" width="428" height="20"/>
</p>
<p class="western">Voici ci-dessous l’algorithme SARSA avec
fonction approximée. On note que même si on écrit s, l’algorithme
manipule en fait le vecteur des attributs observés X(s)&nbsp;:</p>
<ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	But&nbsp;: Trouver une politique quasi-optimale<img src="RL_html_c05e4f8a360ff1d3.gif" name="Object47" hspace="8" width="132" height="20"/>
.</p>
	<ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Initialiser un vecteur <font face="CMU Sans Serif">θ</font>
		aléatoirement&nbsp;;</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		π  ← Une politique qui Choisit pour chaque état s,
		l’action<img src="RL_html_ba9e71435ac946c2.gif" name="Object48" hspace="8" width="119" height="29"/>
avec
		probabilité 
		<img src="RL_html_50c5573d260148aa.gif" name="Object49" hspace="8" width="48" height="19"/>

		ou bien une action aléatoire avec probabilité ε.</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		s ← l’état initial&nbsp;;</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		a ← une action choisie de s par la politique π&nbsp;;</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Répéter&nbsp;:</p>
		<ul>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Effectuer l’action a et observer le gain r et l’état s’&nbsp;;</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			a’ ← une action choisie de s’ par la politique π&nbsp;;</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Calculer l’erreur entre la valeur prévue et le nouvel
			estimateur&nbsp;:</p>
			<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			<img src="RL_html_ed952132f9477926.gif" name="Object50" hspace="8" width="277" height="19"/>
</p>
			<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			<img src="RL_html_66c54f07296f0cc5.gif" name="Object51" hspace="8" width="291" height="19"/>
</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Mettre à jour les composantes <font face="CMU Sans Serif">θ</font><sub>i</sub>
			de sorte à réduire l’erreur&nbsp;:</p>
			<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			<img src="RL_html_368aa687c962b8da.gif" name="Object52" hspace="8" width="227" height="42"/>
</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Faire décroitre la valeur de α&nbsp;;</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			s ← s’&nbsp;; a ← a’&nbsp;;</p>
		</ul>
	</ul>
</ul>
<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
<br/>

</p>
<p class="western">Si la fonction<img src="RL_html_8a1a3b14db8e5fb0.gif" name="Object53" hspace="8" width="68" height="19"/>
est
linéaire, les résultats de convergence (pour le SARSA tabulaire)
restent valides.</p>
<h1 class="western" style="page-break-before: always"><a name="__RefHeading___Toc1523_834395304"></a>
7.SATSA(λ) avec traces d’éligibilité</h1>
<p class="western">L’algorithme SARSA (ou SARSA(0)), améliore son
estimation de la valeur d’état-action 
<img src="RL_html_37112db7c6cfcc01.gif" name="Object55" hspace="8" width="65" height="20"/>

à partir d’une observation R<sub>t+1</sub> et de son estimation du
couple état-action suivant&nbsp;:</p>
<p class="western" align="center"><img src="RL_html_bfdff57e4068abb2.gif" name="Object57" hspace="8" width="311" height="20"/>
</p>
<p class="western">Cette estimation peut être améliorée en voyant
quelques étapes de plus vers le futur&nbsp;:</p>
<p class="western" align="center"><img src="RL_html_f923396a1351d4dc.gif" name="Object56" hspace="8" width="484" height="22"/>
</p>
<p class="western">Cette extension de SARSA (qu’on appelle
n-step-SARSA) peut être implémentée en stockant à tout instant t,
les n derniers couples état-action et les n derniers gains obtenus,
et en misant à jour (à l’instant t) l’estimation de<img src="RL_html_f25cc8e25bb76528.gif" name="Object58" hspace="8" width="91" height="20"/>
.Cet
algorithme génère de meilleures estimations, mais il est moins
réactif vu que la valeur du couple état-action n’est mise à jour
que n étapes après sa visite.</p>
<p class="western">La technique de la <b>trace d’éligibilité</b>
permet de générer de bonnes estimations comme l’algorithme
n-step-SARSA tout en gardant la réactivité de SARSA. L’idée est
de maintenir un vecteur d’éligibilité e de même taille que <font face="CMU Sans Serif">θ</font>.
Le vecteur e est une mémoire à cours terme qui représente la
contribution de chaque composante <font face="CMU Sans Serif">θ</font><sub>i</sub>
au dernier gain généré et donc à l’erreur entre la nouvelle et
l’ancienne estimation. Le vecteur e est initialisé à 0, et est
mit à jour par la formule 
<img src="RL_html_b6bc247b99f391b9.gif" name="Object59" hspace="8" width="191" height="42"/>
(λ
est un paramètre réel entre 0 et 1). La mise à jour des
composantes <font face="CMU Sans Serif">θ</font><sub><font face="CMU Sans Serif">i</font></sub>
de l’algorithme SARSA est remplacée par 
<img src="RL_html_ce9b3702666fc2de.gif" name="Object60" hspace="8" width="148" height="20"/>
.
On appelle ce nouvel algorithme SARSA(λ). Clairement, si  λ = 0, on
obtient l’algorithme SARSA.</p>
<h1 class="western"><br/>
<br/>

</h1>
<h1 class="western" style="page-break-before: always"><a name="__RefHeading___Toc1441_1845084985"></a>
8.Application sur R</h1>
<p class="western">Le package MDPtoolbox du langage R propose des
fonctions liées à la résolution des processus de décision
Markoviens (MDP) à temps discrets: horizon fini, itération de la
valeur, itération de la politique, algorithmes de programmation
linéaire avec quelques variantes et propose également certaines
fonctions liées à l'apprentissage par renforcement.</p>
<p class="western"><a name="result_box1"></a>Cependant je n’ai pas
trouvé sur Internet des exemples d’apprentissage par renforcement
sous R. Pour valider se que j’ai appris, j’ai créé une petite
application avec le langage Javascript que je <span lang="fr-FR">connais
</span><span lang="fr-FR">mieux que</span><span lang="fr-FR"> R.</span></p>
<h1 class="western"><a name="__RefHeading___Toc1525_834395304"></a>9.Application
Javascript</h1>
<p class="western"><a name="result_box2"></a>J’ai implémenté une
simulation du problème «&nbsp;Mountain–Car Task&nbsp;» comme
expliqué dans [1]&nbsp;:</p>
<p class="western"><font color="#ffffff">
  <img src="RL_html_d976b0629cc9dc51.png" name="Image4" align="left" width="205" height="257" border="5"/>
</font>
«&nbsp;Considérez
la tâche de conduire une voiture sur une route de montagne abrupte,
[comme le suggère la figure à droite]. La difficulté est que la
gravité est plus forte que le moteur de la voiture, et même à
pleine accélération, la voiture ne peut pas atteindre le but [qui
est au sommet de la montagne droite]. La seule solution consiste à
s'éloigner du but et à monter la pente opposée à gauche. Ensuite,
en appliquant plein gaz la voiture peut accumuler assez d'inertie
pour atteindre le but. Il s'agit d'un exemple simple d'une tâche où
les choses doivent s'aggraver dans un sens (loin du but) avant
qu'elles ne puissent s'améliorer.&nbsp;»</p>
<h2 class="western"><a name="__RefHeading___Toc1654_881107470"></a>9.1.Modélisation</h2>
<p class="western">Note&nbsp;: l’indexation commence de 0.
C’est-à-dire que <font face="CMU Sans Serif">θ</font><sub>0</sub>
est la première composante du vecteur <font face="CMU Sans Serif">θ</font>.</p>
<ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<b>L’agent</b> est la voiture&nbsp;;</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<b>Les actions</b> possibles sont&nbsp;:</p>
	<ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Accélérer vers la gauche (A = 0)&nbsp;;</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Accélérer vers la droite (A = 1) et</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Ne rien faire (A = 2).</p>
	</ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<b>L’environnement</b>&nbsp;:</p>
	<ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		La voiture est sous l’influence des forces suivantes&nbsp;:</p>
		<ul>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Son accélération&nbsp;;</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			La gravité&nbsp;;</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			La réaction de la route et</p>
			<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
			Le frottement.</p>
		</ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		À l’arrivé au but, la voiture est instantanément téléportée
		vers sa position initiale avec accélération et vitesse nulles.</p>
	</ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<b>Le feature vector</b>&nbsp;:</p>
	<ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		On suppose que l’agent ne peut sentir que sa vitesse et sa
		position horizontales. Les valeurs réelles possibles pour la
		position sont partitionnées en 20 classes. Les valeurs réelles
		possibles pour la vitesse sont eux-aussi partitionnées en 20
		classes.</p>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Tout état S est résumé par un couple (position, vitesse) qui est
		transformé à un feature vector X(S) de 400 booléens contenant
		des zéros partout et un 1 à la composante<img src="RL_html_385074a17273598d.gif" name="Object54" hspace="8" width="291" height="19"/>
.</p>
	</ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<b>La fonction approximée</b>&nbsp;:</p>
	<ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		On défini un vecteur <font face="CMU Sans Serif">θ</font>
		de<img src="RL_html_ee331066855aa5ad.gif" name="Object61" hspace="8" width="54" height="18"/>
composantes
		et la fonction approximée<img src="RL_html_4c2a946399347bf1.gif" name="Object62" hspace="8" width="74" height="19"/>
comme
		suit&nbsp;:</p>
		<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		<img src="RL_html_afa2fe5d8a1c981c.gif" name="Object63" hspace="8" width="231" height="33"/>
</p>
		<p class="western" align="center" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		<img src="RL_html_e24b80c87a14c32f.gif" name="Frame4" alt="Frame4" align="bottom"/>
</p>
	</ul>
	<p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<b>La granularité temporelle</b>&nbsp;:</p>
	<ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		L’agent choisit une action à chaque changement du feature-vector
		X(S<sub>t</sub>) ou bien s’il n’y as pas de changement pendant
		250ms.</p>
	</ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	<b>La distribution des gains</b>&nbsp;:</p>
	<ul>
		<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
		Avant que l’agent exécute sa nouvelle décision, il reçoit un
		gain = -1 sauf s’il arrive au but au quel cas il obtient un gain
		= +1.</p>
	</ul>
</ul>
<p class="western"><br/>
<br/>

</p>
<h2 class="western"><a name="__RefHeading___Toc1735_881107470"></a>9.2.Résultat</h2>
<p class="western">On remarque qu’avec l’algorithme SARSA(λ)
avec comme paramètres (α=0.1, γ=0.9, ε=0.05, λ=0.95) et après
plusieurs essais, la voiture trouve une politique lui permettant
d’arriver au but dans un temps compétitif à celui d’un joueur
humain&nbsp;:</p>
<ul>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Le fichier <b><a href="./jeu.html">jeu.html</a></b> contient une version de la simulation
	jouable par l’être humain.</p>
	<li><p class="western" align="left" style="margin-bottom: 0in; line-height: 100%; page-break-before: auto">
	Le fichier <b><a href="./selfplay.html">selfplay.html</a></b> contient la simulation avec
	apprentissage automatique.</p>
</ul>
<p class="western"><br/>
<br/>

</p>
<h1 class="western"><a name="__RefHeading___Toc1656_881107470"></a>10.Logiciels
d’apprentissage par renforcement</h1>
<p class="western">Tous les logiciels que j’ai trouvés sont
single-purpose et ne sont pas généraux.</p>
<h1 class="western"><br/>
<br/>

</h1>
<h1 class="western" style="page-break-before: always"><a name="__RefHeading___Toc1737_881107470"></a>
11.Conclusion</h1>
<p class="western">L’apprentissage par renforcement est un sous
domaine de l’apprentissage machine où un agent interagit avec son
environnement et apprend à choisir les actions qui maximisent ses
gains.</p>
<p class="western">Quand l’environnement est très complexe,
l’apprentissage par renforcement utilise les méthodes
d’apprentissage supervisé (en particulier les méthodes de
régression) pour estimer la valeur des actions.</p>
<p class="western">Nous avons présenté ici uniquement l’algorithme
SARSA(λ) qui est une méthode de différences temporelles de type
«&nbsp;on-policy&nbsp;». Le domaine d’apprentissage par
renforcement est bien plus vaste.</p>
<p class="western"><br/>
<br/>

</p>
<p class="western"><br/>
<br/>

</p>
<p class="western"><br/>
<br/>

</p>
<p class="western"><br/>
<br/>

</p>
<p class="western" align="center"><img src="RL_html_31834105ed420dc.png" name="Image6" align="bottom" width="367" height="346" border="0"/>
</p>
<h1 class="western"><br/>
<br/>

</h1>
<h1 class="western" style="page-break-before: always"><a name="__RefHeading___Toc1658_881107470"></a>
12.Références</h1>
<table width="100%" cellpadding="0" cellspacing="0">
	<col width="12*"/>

	<col width="244*"/>

	<tr valign="top">
		<td width="5%" style="border: none; padding: 0in"><p class="western" align="justify">
			[1]</p>
		</td>
		<td width="95%" style="border: none; padding: 0in"><p class="western" align="justify">
			Sutton, Richard S., and Andrew G. Barto. Reinforcement learning:
			An introduction. Vol. 1. No. 1. Cambridge: MIT press, 1998.</p>
		</td>
	</tr>
	<tr valign="top">
		<td width="5%" style="border: none; padding: 0in"><p class="western" align="justify">
			[2]</p>
		</td>
		<td width="95%" style="border: none; padding: 0in"><p class="western" align="justify">
			SYS, T., and É. ATOIR. &quot;APPRENTISSAGE PAR RENFORCEMENT: UN
			TUTORIEL.&quot;</p>
		</td>
	</tr>
	<tr valign="top">
		<td width="5%" style="border: none; padding: 0in"><p class="western" align="justify">
			[3]</p>
		</td>
		<td width="95%" style="border: none; padding: 0in"><p class="western" align="justify">
			Sutton, Richard S., Tutorial: Introduction to Reinforcement
			Learning with Function Approximation, Microsoft research, 
			<a href="https://www.youtube.com/watch?v=ggqnxyjaKe4">https://www.youtube.com/watch?v=ggqnxyjaKe4</a></p>
		</td>
	</tr>
	<tr valign="top">
		<td width="5%" style="border: none; padding: 0in"><p class="western" align="justify">
			[4]</p>
		</td>
		<td width="95%" style="border: none; padding: 0in"><p class="western" align="justify">
			David Silver RL Course - Lectures 1 to 10: Introduction to
			Reinforcement Learning,
			<a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">https://www.youtube.com/watch?v=2pWv7GOvuf0</a></p>
		</td>
	</tr>
</table>
<p class="western"><br/>
<br/>

</p>
</body>
</html>
